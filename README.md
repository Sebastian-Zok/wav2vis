# wav2vis

**AI Model for Audio to Viseme Translation**

This project implements an AI model for converting audio speech into viseme sequences, enabling realistic lip-sync animation from speech audio for cartoon animations.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Project Structure](#project-structure)
- [Setup & Installation](#setup--installation)
- [Usage Pipeline](#usage-pipeline)
- [Model Architecture](#model-architecture)
- [Requirements](#requirements)

## ğŸ¯ Project Overview

This project uses a teacher-student distillation approach to create an efficient audio-to-viseme translation model:

- **Teacher Model**: A larger, more accurate phoneme recognition model
- **Student Model**: A smaller, optimized model for real-time inference
- **Dataset**: Mozilla Common Voice corpus with forced alignment for phoneme-to-viseme mapping

## ğŸ“ Project Structure

### Current Structure

```
commonVoiceDataset/
â”œâ”€â”€ ğŸ“‚ data/                    # Original Mozilla Common Voice data and generated feature files
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ statistics/
â”œâ”€â”€ ğŸ““ notebooks/               # Jupyter notebooks
â”œâ”€â”€ ğŸ src/                     # Source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ convert_to_mp3.py
â”‚   â”‚   â”œâ”€â”€ get_gentle_alignments.py
â”‚   â”‚   â””â”€â”€ gentle_to_phn.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ teacher_model.py
â”‚   â”‚   â””â”€â”€ student_model.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”‚   â”œâ”€â”€ audio_utils.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Setup & Installation

### Prerequisites

- Python 3.8+
- Docker (for Gentle forced aligner)
- CUDA-capable GPU (recommended for training)

### Quick Start

```bash
# Create virtual environment
py -3.11 -m venv tf-env# On Windows:
.venv\Scripts\activate
# On Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Gentle Forced Aligner Setup

```bash
# Pull and run Gentle Docker container
docker pull lowerquality/gentle
docker run -p 8765:8765 lowerquality/gentle
```

## ğŸš€ Usage Pipeline

### 1. ğŸ”Š Audio Alignment

```python
# Extract phoneme alignments using Gentle
python get_gentle_alignments.py
python gentle_to_phn.py
```

### 2. ğŸ“¥ Data Preparation

```python
# Run DataPreperation.ipynb
# - Removes OOV (Out-of-Vocabulary) sentences
# - Cleans and filters dataset
```

### 3. ğŸ›ï¸ Feature Extraction

```python
# Run feature_extraction.ipynb
# - Extract MFCC features
# - Generate spectrograms
# - Create feature vectors for training
```

### 4. ğŸ“Š Data Analysis & Normalization

```python
# Run Analyze.ipynb
# - Analyze feature distributions
# - Compute normalization statistics
# - Prepare data for training
```

### 5. ğŸ“ Teacher Model Training

```python
# Run Training.ipynb
# - Train the teacher phoneme recognition model
# - Generate teacher logits for distillation
```

### 6. ğŸ’ Student Model Distillation

```python
# Run Distillation Script.ipynb
# - Train compressed student model
# - Use teacher knowledge distillation
# - Optimize for real-time inference
```

### 7. ğŸ§ª Model Evaluation

```python
# Run Test.ipynb
# - Evaluate model performance
# - Test on validation set
# - Generate performance metrics
```

## ğŸ—ï¸ Model Architecture

### Teacher Model

- **Input**: MFCC features (13 coefficients + derivatives)
- **Architecture**: Deep CNN-LSTM network
- **Output**: Phoneme probabilities
- **Purpose**: High-accuracy phoneme recognition

### Student Model

- **Input**: Same MFCC features
- **Architecture**: Lightweight CNN network
- **Output**: Viseme classifications
- **Purpose**: Real-time audio-to-viseme translation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request
